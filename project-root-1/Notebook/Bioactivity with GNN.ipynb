{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6cbca7a6-f7f4-4ebb-98ce-e2900a4bc752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and set up\n",
    "import os, math, json, random, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import optuna\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92aae568-c634-47f9-8fbb-73f8b84c1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "# --- Fix seeds for reproducibility ---\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Python & NumPy\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch (CPU & GPU/MPS)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure deterministic behavior (slower but reproducible)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "# ðŸ”¹ Use a seeded generator for DataLoader shuffling\n",
    "g = torch.Generator().manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8660f86e-d543-4924-b499-cfab1d0fb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "target   = \"CHEMBL203\"   # EGFR\n",
    "endpoint = \"IC50\"\n",
    "outdir   = \"outputs/egfr_ic50_scaffold\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "FP_BITS   = 1024\n",
    "FP_RADIUS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fce603f0-3f92-4932-bbd2-5583bb311f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpers\n",
    "UNIT_TO_M = {\"M\":1,\"mM\":1e-3,\"uM\":1e-6,\"ÂµM\":1e-6,\"nM\":1e-9,\"pM\":1e-12}\n",
    "\n",
    "def to_molar(val, units):\n",
    "    try: return float(val) * UNIT_TO_M.get(units, np.nan)\n",
    "    except: return None\n",
    "\n",
    "def to_pIC50(molar):\n",
    "    return -math.log10(molar) if molar and molar > 0 else None\n",
    "\n",
    "def clean_smiles(smi):\n",
    "    mol = Chem.MolFromSmiles(smi) if isinstance(smi,str) else None\n",
    "    if mol is None: return None\n",
    "    lfc = rdMolStandardize.LargestFragmentChooser(preferOrganic=True)\n",
    "    mol = lfc.choose(mol)\n",
    "    mol = rdMolStandardize.Uncharger().uncharge(mol)\n",
    "    try: Chem.SanitizeMol(mol)\n",
    "    except: return None\n",
    "    return Chem.MolToSmiles(mol, canonical=True)\n",
    "\n",
    "def scaffold_split(df, frac_train=0.8, seed=42):\n",
    "    \"\"\"Split dataset into train/test by Bemisâ€“Murcko scaffolds.\"\"\"\n",
    "    scaffolds = {}\n",
    "    for idx, smi in enumerate(df[\"clean_smiles\"]):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        scaffold = MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=False)\n",
    "        scaffolds.setdefault(scaffold, []).append(idx)\n",
    "\n",
    "    # sort by scaffold size\n",
    "    sorted_scaffolds = sorted(scaffolds.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(sorted_scaffolds)\n",
    "\n",
    "    n_total = len(df)\n",
    "    n_train = int(frac_train * n_total)\n",
    "    train_idx, test_idx = [], []\n",
    "\n",
    "    for scaffold, idxs in sorted_scaffolds:\n",
    "        if len(train_idx) + len(idxs) <= n_train:\n",
    "            train_idx.extend(idxs)\n",
    "        else:\n",
    "            test_idx.extend(idxs)\n",
    "\n",
    "    return df.iloc[train_idx], df.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af663b9d-34ee-43ca-a15b-c4be40fc033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Fetching data...\n",
      "Raw shape: (18105, 5)\n"
     ]
    }
   ],
   "source": [
    "#Fetch ChEMBL data\n",
    "print(\"ðŸ“¥ Fetching data...\")\n",
    "acts = new_client.activity.filter(target_chembl_id=target, standard_type=endpoint)\n",
    "# keep only high-confidence assays\n",
    "acts = acts.filter(assay_confidence_score__gte=8)  \n",
    "rows = [{\"molecule_chembl_id\":r.get(\"molecule_chembl_id\"),\n",
    "         \"canonical_smiles\":r.get(\"canonical_smiles\"),\n",
    "         \"standard_value\":r.get(\"standard_value\"),\n",
    "         \"standard_units\":r.get(\"standard_units\"),\n",
    "         \"standard_relation\":r.get(\"standard_relation\")} for r in acts]\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Raw shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4f648e8-cd74-407c-ac95-fd53ad88f57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: (7909, 3)\n"
     ]
    }
   ],
   "source": [
    "#Cleaning and transformation\n",
    "df = df[df[\"standard_relation\"].isin([\"=\",\"~\",\"â‰ˆ\",None])]\n",
    "df[\"clean_smiles\"] = df[\"canonical_smiles\"].apply(clean_smiles)\n",
    "df[\"molar\"] = df.apply(lambda r: to_molar(r[\"standard_value\"], r[\"standard_units\"]), axis=1)\n",
    "df[\"pIC50\"] = df[\"molar\"].apply(to_pIC50)\n",
    "df = df.dropna(subset=[\"clean_smiles\",\"pIC50\"]).reset_index(drop=True)\n",
    "# ðŸ”¹ aggregate duplicates: median pIC50 per molecule\n",
    "df = df.groupby([\"molecule_chembl_id\", \"clean_smiles\"], as_index=False)[\"pIC50\"].median()\n",
    "print(\"After cleaning:\", df.shape)\n",
    "df.to_csv(os.path.join(outdir,\"processed.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cef273bf-f798-4150-acc7-8da8eb7997bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Featurizing molecules...\n",
      "Feature matrix: (7909, 1241)\n"
     ]
    }
   ],
   "source": [
    "#Featurisation (Fingerprint and descriptors)\n",
    "print(\"ðŸ”¬ Featurizing molecules...\")\n",
    "mols = [Chem.MolFromSmiles(s) for s in df[\"clean_smiles\"]]\n",
    "\n",
    "# Morgan FP\n",
    "gen = GetMorganGenerator(radius=FP_RADIUS, fpSize=FP_BITS)\n",
    "def fp_array(m): \n",
    "    arr = np.zeros((FP_BITS,), dtype=int)\n",
    "    if m: Chem.DataStructs.ConvertToNumpyArray(gen.GetFingerprint(m), arr)\n",
    "    return arr\n",
    "fps = np.array([fp_array(m) for m in mols], dtype=int)\n",
    "\n",
    "# RDKit descriptors\n",
    "desc_funcs = [f for _,f in Descriptors._descList]\n",
    "def desc_array(m):\n",
    "    return np.array([f(m) for f in desc_funcs], dtype=float) if m else np.zeros((len(desc_funcs),))\n",
    "desc = np.array([desc_array(m) for m in mols], dtype=float)\n",
    "\n",
    "# Combine\n",
    "X = np.hstack([fps, desc])\n",
    "y = df[\"pIC50\"].values.astype(float)\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"constant\", fill_value=0.0)\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "print(\"Feature matrix:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6fe5c736-1204-49b7-8120-ce4cc559f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classical Models\n",
    "metrics = {}\n",
    "\n",
    "# --- Scaffold-based K-Fold Cross Validation ---\n",
    "def scaffold_kfold(df, X, y, n_splits=5, seed=42):\n",
    "    scaffolds = {}\n",
    "    for idx, smi in enumerate(df[\"clean_smiles\"]):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        scaffold = MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=False)\n",
    "        scaffolds.setdefault(scaffold, []).append(idx)\n",
    "\n",
    "    all_scaffolds = list(scaffolds.values())\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(all_scaffolds)\n",
    "\n",
    "    folds = [[] for _ in range(n_splits)]\n",
    "    for i, group in enumerate(all_scaffolds):\n",
    "        folds[i % n_splits].extend(group)\n",
    "\n",
    "    for k in range(n_splits):\n",
    "        test_idx = folds[k]\n",
    "        train_idx = [i for j in range(n_splits) if j != k for i in folds[j]]\n",
    "        yield train_idx, test_idx\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# --- RandomForest CV ---\n",
    "rf_rmse, rf_mae, rf_r2 = [], [], []\n",
    "for train_idx, test_idx in scaffold_kfold(df, X, y, n_splits=5):\n",
    "    rf = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X[train_idx], y[train_idx])\n",
    "    pred = rf.predict(X[test_idx])\n",
    "    rf_rmse.append(np.sqrt(mean_squared_error(y[test_idx], pred)))\n",
    "    rf_mae.append(mean_absolute_error(y[test_idx], pred))\n",
    "    rf_r2.append(r2_score(y[test_idx], pred))\n",
    "metrics[\"RandomForest_CV\"] = {\n",
    "    \"RMSE\": np.mean(rf_rmse),\n",
    "    \"MAE\": np.mean(rf_mae),\n",
    "    \"R2\": np.mean(rf_r2)\n",
    "}\n",
    "\n",
    "# --- SVR CV ---\n",
    "svr_rmse, svr_mae, svr_r2 = [], [], []\n",
    "for train_idx, test_idx in scaffold_kfold(df, X, y, n_splits=5):\n",
    "    svr = make_pipeline(StandardScaler(), SVR(C=10.0, epsilon=0.1, kernel=\"rbf\"))\n",
    "    svr.fit(X[train_idx], y[train_idx])\n",
    "    pred = svr.predict(X[test_idx])\n",
    "    svr_rmse.append(np.sqrt(mean_squared_error(y[test_idx], pred)))\n",
    "    svr_mae.append(mean_absolute_error(y[test_idx], pred))\n",
    "    svr_r2.append(r2_score(y[test_idx], pred))\n",
    "metrics[\"SVR_CV\"] = {\n",
    "    \"RMSE\": np.mean(svr_rmse),\n",
    "    \"MAE\": np.mean(svr_mae),\n",
    "    \"R2\": np.mean(svr_r2)\n",
    "}\n",
    "\n",
    "# --- XGBoost CV ---\n",
    "xgb_rmse, xgb_mae, xgb_r2 = [], [], []\n",
    "for train_idx, test_idx in scaffold_kfold(df, X, y, n_splits=5):\n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X[train_idx], y[train_idx])\n",
    "    pred = model.predict(X[test_idx])\n",
    "    xgb_rmse.append(np.sqrt(mean_squared_error(y[test_idx], pred)))\n",
    "    xgb_mae.append(mean_absolute_error(y[test_idx], pred))\n",
    "    xgb_r2.append(r2_score(y[test_idx], pred))\n",
    "metrics[\"XGBoost_CV\"] = {\n",
    "    \"RMSE\": np.mean(xgb_rmse),\n",
    "    \"MAE\": np.mean(xgb_mae),\n",
    "    \"R2\": np.mean(xgb_r2)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b0521-ff09-498c-8beb-f81867d39d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classification labels ---\n",
    "# Define active if IC50 <= 1 ÂµM (pIC50 >= 6.0)\n",
    "df[\"active\"] = (df[\"pIC50\"] >= 6.0).astype(int)\n",
    "y_cls = df[\"active\"].values\n",
    "\n",
    "# --- RandomForest Classifier CV ---\n",
    "rf_auc, rf_acc, rf_f1 = [], [], []\n",
    "for train_idx, test_idx in scaffold_kfold(df, X, y_cls, n_splits=5):\n",
    "    clf = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "    clf.fit(X[train_idx], y_cls[train_idx])\n",
    "    probas = clf.predict_proba(X[test_idx])[:,1]\n",
    "    preds = clf.predict(X[test_idx])\n",
    "    rf_auc.append(roc_auc_score(y_cls[test_idx], probas))\n",
    "    rf_acc.append(accuracy_score(y_cls[test_idx], preds))\n",
    "    rf_f1.append(f1_score(y_cls[test_idx], preds))\n",
    "metrics[\"RandomForest_Cls\"] = {\n",
    "    \"ROC-AUC\": np.mean(rf_auc),\n",
    "    \"Accuracy\": np.mean(rf_acc),\n",
    "    \"F1\": np.mean(rf_f1)\n",
    "}\n",
    "\n",
    "# --- SVC CV ---\n",
    "svr_auc, svr_acc, svr_f1 = [], [], []\n",
    "for train_idx, test_idx in scaffold_kfold(df, X, y_cls, n_splits=5):\n",
    "    clf = make_pipeline(StandardScaler(), SVC(C=10.0, kernel=\"rbf\", probability=True))\n",
    "    clf.fit(X[train_idx], y_cls[train_idx])\n",
    "    probas = clf.predict_proba(X[test_idx])[:,1]\n",
    "    preds = clf.predict(X[test_idx])\n",
    "    svr_auc.append(roc_auc_score(y_cls[test_idx], probas))\n",
    "    svr_acc.append(accuracy_score(y_cls[test_idx], preds))\n",
    "    svr_f1.append(f1_score(y_cls[test_idx], preds))\n",
    "metrics[\"SVC_Cls\"] = {\n",
    "    \"ROC-AUC\": np.mean(svr_auc),\n",
    "    \"Accuracy\": np.mean(svr_acc),\n",
    "    \"F1\": np.mean(svr_f1)\n",
    "}\n",
    "\n",
    "# --- XGB Classifier CV ---\n",
    "xgb_auc, xgb_acc, xgb_f1 = [], [], []\n",
    "for train_idx, test_idx in scaffold_kfold(df, X, y_cls, n_splits=5):\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "        tree_method=\"hist\", use_label_encoder=False, eval_metric=\"logloss\"\n",
    "    )\n",
    "    clf.fit(X[train_idx], y_cls[train_idx])\n",
    "    probas = clf.predict_proba(X[test_idx])[:,1]\n",
    "    preds = clf.predict(X[test_idx])\n",
    "    xgb_auc.append(roc_auc_score(y_cls[test_idx], probas))\n",
    "    xgb_acc.append(accuracy_score(y_cls[test_idx], preds))\n",
    "    xgb_f1.append(f1_score(y_cls[test_idx], preds))\n",
    "metrics[\"XGBoost_Cls\"] = {\n",
    "    \"ROC-AUC\": np.mean(xgb_auc),\n",
    "    \"Accuracy\": np.mean(xgb_acc),\n",
    "    \"F1\": np.mean(xgb_f1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354fb7a2-585f-44cd-bfb8-568cb87d8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GNN Model\n",
    "print(\"ðŸ§  Training GNN...\")\n",
    "\n",
    "def mol_to_graph(mol, y_val):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Node features (atoms)\n",
    "    x = []\n",
    "    for a in mol.GetAtoms():\n",
    "        x.append([\n",
    "            a.GetAtomicNum(),\n",
    "            a.GetDegree(),\n",
    "            a.GetTotalNumHs(),\n",
    "            a.GetImplicitValence(),\n",
    "            int(a.GetIsAromatic()),\n",
    "            a.GetFormalCharge(),\n",
    "            a.GetHybridization()\n",
    "        ])\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "    # Edge features (bonds)\n",
    "    ei = [[], []]\n",
    "    eb = []\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bond_type = b.GetBondTypeAsDouble()\n",
    "        ei[0] += [i, j]\n",
    "        ei[1] += [j, i]\n",
    "        eb.append([bond_type])\n",
    "        eb.append([bond_type])\n",
    "    \n",
    "    edge_index = torch.tensor(ei, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(eb, dtype=torch.float)\n",
    "\n",
    "    y_t = torch.tensor([y_val], dtype=torch.float)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y_t)\n",
    "\n",
    "graphs = [mol_to_graph(m,yv) for m,yv in zip(mols,y)]\n",
    "graphs = [g for g in graphs if g]\n",
    "\n",
    "# --- GNN uses a single scaffold split ---\n",
    "train_df, test_df = scaffold_split(df, frac_train=0.8)\n",
    "\n",
    "train_graphs = [graphs[i] for i in train_df.index]\n",
    "test_graphs  = [graphs[i] for i in test_df.index]\n",
    "\n",
    "train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True, generator=g)\n",
    "test_loader  = DataLoader(test_graphs, batch_size=64, generator=g)\n",
    "\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
    "\n",
    "class GNNRegressor(nn.Module):\n",
    "    def __init__(self, in_feats=7, hidden=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden)\n",
    "        self.bn1 = BatchNorm(hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.bn2 = BatchNorm(hidden)\n",
    "        self.lin1 = nn.Linear(hidden, hidden//2)\n",
    "        self.lin2 = nn.Linear(hidden//2, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.lin2(x).squeeze(-1)\n",
    "\n",
    "device=torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "gnn=GNNRegressor().to(device)\n",
    "opt=torch.optim.Adam(gnn.parameters(),lr=1e-3)\n",
    "loss_fn=nn.MSELoss()\n",
    "\n",
    "EPOCHS = 200\n",
    "patience = 20\n",
    "best_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "#Define the list \n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    # ----- Training -----\n",
    "    gnn.train(); total=0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {ep+1}/{EPOCHS}\"):\n",
    "        batch = batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = gnn(batch.x, batch.edge_index, batch.batch, getattr(batch, \"edge_attr\", None))\n",
    "        loss = loss_fn(pred, batch.y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    avg_train_loss = total / len(train_loader)\n",
    "\n",
    "    # ----- Validation -----\n",
    "    gnn.eval(); val_loss=0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = gnn(batch.x, batch.edge_index, batch.batch, getattr(batch, \"edge_attr\", None))\n",
    "            val_loss += loss_fn(pred, batch.y).item()\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    # ðŸ”¹ Save for learning curve\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {ep+1} | train {avg_train_loss:.4f} | val {avg_val_loss:.4f}\")\n",
    "\n",
    "    # ----- Early Stopping -----\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(gnn.state_dict(), os.path.join(outdir, \"best_gnn.pt\"))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"â¹ï¸ Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "gnn.load_state_dict(torch.load(os.path.join(outdir, \"best_gnn.pt\")))\n",
    "\n",
    "gnn.eval(); y_true=[]; y_pred=[]\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating GNN\"):\n",
    "        batch=batch.to(device)\n",
    "        pred=gnn(batch.x,batch.edge_index,batch.batch)\n",
    "        y_true+=batch.y.cpu().numpy().tolist()\n",
    "        y_pred+=pred.cpu().numpy().tolist()\n",
    "metrics[\"GNN\"]={\"RMSE\":float(np.sqrt(mean_squared_error(y_true,y_pred))),\n",
    "                \"MAE\": float(mean_absolute_error(y_true,y_pred)),\n",
    "                \"R2\":  float(r2_score(y_true,y_pred))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb5e9a-bbc1-4524-bd36-aa39385871c6",
   "metadata": {},
   "outputs": [],
   "source": [
    " #--- GNN Classification ---\n",
    "print(\"ðŸ§  Training GNN Classifier...\")\n",
    "\n",
    "# Define binary labels (active if pIC50 >= 6.0)\n",
    "df[\"active\"] = (df[\"pIC50\"] >= 6.0).astype(int)\n",
    "y_cls = df[\"active\"].values\n",
    "\n",
    "# Build graphs for classification\n",
    "graphs_cls = [mol_to_graph(m, yv) for m, yv in zip(mols, y_cls)]\n",
    "graphs_cls = [g for g in graphs_cls if g]\n",
    "\n",
    "# Scaffold split\n",
    "train_df_cls, test_df_cls = scaffold_split(df, frac_train=0.8)\n",
    "train_graphs_cls = [graphs_cls[i] for i in train_df_cls.index]\n",
    "test_graphs_cls  = [graphs_cls[i] for i in test_df_cls.index]\n",
    "\n",
    "train_loader_cls = DataLoader(train_graphs_cls, batch_size=32, shuffle=True, generator=g)\n",
    "test_loader_cls  = DataLoader(test_graphs_cls, batch_size=64, generator=g)\n",
    "\n",
    "# --- GNN Classifier Model ---\n",
    "class GNNClassifier(nn.Module):\n",
    "    def __init__(self, in_feats=7, hidden=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden)\n",
    "        self.bn1 = BatchNorm(hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.bn2 = BatchNorm(hidden)\n",
    "        self.lin1 = nn.Linear(hidden, hidden//2)\n",
    "        self.lin2 = nn.Linear(hidden//2, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return torch.sigmoid(self.lin2(x)).squeeze(-1)\n",
    "\n",
    "# Device + optimizer\n",
    "gnn_cls = GNNClassifier().to(device)\n",
    "opt_cls = torch.optim.Adam(gnn_cls.parameters(), lr=1e-3)\n",
    "loss_fn_cls = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 200\n",
    "patience = 15\n",
    "best_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "train_losses_cls, val_losses_cls = [], []\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    # Training\n",
    "    gnn_cls.train(); total=0\n",
    "    for batch in tqdm(train_loader_cls, desc=f\"Epoch {ep+1}/{EPOCHS} (Cls)\"):\n",
    "        batch = batch.to(device)\n",
    "        opt_cls.zero_grad()\n",
    "        pred = gnn_cls(batch.x, batch.edge_index, batch.batch, getattr(batch, \"edge_attr\", None))\n",
    "        loss = loss_fn_cls(pred, batch.y.float())\n",
    "        loss.backward()\n",
    "        opt_cls.step()\n",
    "        total += loss.item()\n",
    "    avg_train_loss = total / len(train_loader_cls)\n",
    "\n",
    "    # Validation\n",
    "    gnn_cls.eval(); val_loss=0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_cls:\n",
    "            batch = batch.to(device)\n",
    "            prob = gnn_cls(batch.x, batch.edge_index, batch.batch, getattr(batch, \"edge_attr\", None))\n",
    "            val_loss += loss_fn_cls(prob, batch.y.float()).item()\n",
    "    avg_val_loss = val_loss / len(test_loader_cls)\n",
    "\n",
    "    # Track\n",
    "    train_losses_cls.append(avg_train_loss)\n",
    "    val_losses_cls.append(avg_val_loss)\n",
    "    print(f\"Epoch {ep+1} | train {avg_train_loss:.4f} | val {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(gnn_cls.state_dict(), os.path.join(outdir, \"best_gnn_cls.pt\"))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"â¹ï¸ Early stopping triggered (Cls)\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "gnn_cls.load_state_dict(torch.load(os.path.join(outdir, \"best_gnn_cls.pt\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f830f-6886-4449-adf0-a315afe5aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both regression + classification metrics\n",
    "with open(os.path.join(outdir, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"âœ… Metrics saved:\")\n",
    "\n",
    "# Print regression and classification separately\n",
    "for model, vals in metrics.items():\n",
    "    if isinstance(vals, dict):  # check if it's a dictionary\n",
    "        if \"RMSE\" in vals and \"MAE\" in vals:  # regression models\n",
    "            print(f\"{model} â†’ RMSE: {vals['RMSE']:.4f}, MAE: {vals['MAE']:.4f}, RÂ²: {vals['R2']:.4f}\")\n",
    "        elif \"ROC-AUC\" in vals:  # classification models\n",
    "            print(f\"{model} â†’ ROC-AUC: {vals['ROC-AUC']:.4f}, Accuracy: {vals['Accuracy']:.4f}, F1: {vals['F1']:.4f}\")\n",
    "        else:\n",
    "            print(f\"{model}: {vals}\")\n",
    "    else:\n",
    "        # vals is just a single number (like mean CV RÂ²)\n",
    "        print(f\"{model}: {vals:.4f}\")\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics).T.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "\n",
    "# --- Evaluation for GNN Classification ---\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n",
    "\n",
    "gnn_cls.eval(); y_true, y_pred, y_prob = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_cls, desc=\"Evaluating GNN Classifier\"):\n",
    "        batch = batch.to(device)\n",
    "        prob = gnn_cls(batch.x, batch.edge_index, batch.batch)\n",
    "        pred = (prob > 0.5).int()\n",
    "        y_true += batch.y.cpu().numpy().tolist()\n",
    "        y_pred += pred.cpu().numpy().tolist()\n",
    "        y_prob += prob.cpu().numpy().tolist()\n",
    "\n",
    "metrics[\"GNN_Cls\"] = {\n",
    "    \"ROC-AUC\": float(roc_auc_score(y_true, y_prob)),\n",
    "    \"PR-AUC\": float(average_precision_score(y_true, y_prob)),\n",
    "    \"Accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "    \"F1\": float(f1_score(y_true, y_pred))\n",
    "}\n",
    "print(\"âœ… GNN Classification Metrics:\", metrics[\"GNN_Cls\"])\n",
    "\n",
    "# --- Plot GNN Classification Metrics ---\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=list(metrics[\"GNN_Cls\"].keys()), y=list(metrics[\"GNN_Cls\"].values()))\n",
    "plt.ylim(0,1)\n",
    "plt.title(\"GNN Classification Performance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outdir, \"gnn_classification_metrics.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(train_losses_cls, label=\"Train Loss\")\n",
    "plt.plot(val_losses_cls, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.title(\"GNN Classification Learning Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outdir, \"gnn_cls_learning_curve.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# --- Regression plots (only for models with RMSE/MAE/R2) ---\n",
    "reg_df = df_metrics.dropna(subset=[\"R2\"], how=\"any\")\n",
    "\n",
    "if not reg_df.empty:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    melt = reg_df.melt(id_vars=\"Model\", value_vars=[\"RMSE\",\"MAE\"], \n",
    "                       var_name=\"Metric\", value_name=\"Value\")\n",
    "    sns.barplot(data=melt, x=\"Model\", y=\"Value\", hue=\"Metric\")\n",
    "    plt.title(\"Regression Error Comparison (RMSE vs MAE)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"errors_bar.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(data=reg_df, x=\"Model\", y=\"R2\")\n",
    "    plt.title(\"Regression RÂ² Comparison\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"r2_bar.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# --- Classification plots (only for models with ROC-AUC/Accuracy/F1) ---\n",
    "cls_df = df_metrics.dropna(subset=[\"ROC-AUC\"], how=\"any\")\n",
    "\n",
    "if not cls_df.empty:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    melt = cls_df.melt(id_vars=\"Model\", value_vars=[\"Accuracy\",\"F1\"], \n",
    "                       var_name=\"Metric\", value_name=\"Value\")\n",
    "    sns.barplot(data=melt, x=\"Model\", y=\"Value\", hue=\"Metric\")\n",
    "    plt.title(\"Classification Accuracy / F1 Comparison\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"cls_acc_f1.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(data=cls_df, x=\"Model\", y=\"ROC-AUC\")\n",
    "    plt.title(\"Classification ROC-AUC Comparison\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, \"cls_auc.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# ðŸ”¹ Plot training vs validation loss curve\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"GNN Learning Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outdir,\"gnn_learning_curve.png\"), dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3851a9-79d6-421b-9f1f-6b68673a8c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chemml]",
   "language": "python",
   "name": "conda-env-chemml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
